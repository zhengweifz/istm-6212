{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 02 - Due Friday, November 11 at 12pm\n",
    "\n",
    "*Objectives*: Gain experience loading a transactional dataset into a relational database model you define yourself and using SQL to explore its contents. Transform the data into a star schema, documenting the schema visually, and explore the transformed data analytically by writing and executing a number of SQL queries using common syntax and functions and describing your findings.  Gain practice working on these tasks with a partner.\n",
    "\n",
    "*Grading criteria*: The tasks should all be completed, and questions should all be answered with SQL queries, diagrams, and charts in the space provided, unless a text answer is requested.  The notebook itself should be completely reproducible at datanotebook.org, from start to finish: another person should be able to use the same code to obtain the same results as yours.  Note that you will receive no more than partial credit if you do not add text/markdown cells explaining your thinking at each major step in each problem.\n",
    "\n",
    "*Attestation*: **Work in pairs**.  At the end of your submitted notebook, identify the work each partner performed and attest that each contributed substantially to the work.\n",
    "\n",
    "*Deadline*: Friday, November 11, 12pm.  One member of each pair must submit your notebook to Blackboard; you should not both submit it separately.  You may each push a copy to your own GitHub repository.\n",
    "\n",
    "\n",
    "## Setup - select and obtain data\n",
    "\n",
    "The US City Open Data Census has a variety of [transactional data from all over the country](http://us-city.census.okfn.org/).  Select one topic and one city from this Census, download the data, and explain your reason for this choice before proceeding.  Provide a link to the web page for the data set you've chosen.\n",
    "\n",
    "I strongly encourage you to select a dataset from among the following topics:  Crime, Property Assessment, Campaign Finance Contributions, Service Requests (311), and Web Analytics.\n",
    "\n",
    "By *transactional* data I mean records at the grain of one event per business process.  In the case of Service Requests (311), for example, that would require data at the level of each individual report of a service request.  If this isn't clear, think about the bike trip data - each individual ride was included - and look for data at that level of event/process specificity.  Avoid data like the Connecticut boating data, which was summarized by year, not individual transactions.\n",
    "\n",
    "Please aim for a dataset of at least 10,000 individual records, but less than 250,000 records.  A little more or a little less is okay, but try to stay within these limits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Santa Monica Sevice Requested Dataset\n",
    "https://data.smgov.net/Public-Services/Closed-GovOutreach-Submissions/tsas-mvez\n",
    "It is a nice transactional dataset. It has 58268 records, meeting the size requirement. Futhermore, the Santa Monica website provides a nice API for exporting the dataset in a csv format. There at least two dimensions in this dataset, location and time. The \"Days to Respond\" column is a measurement column in this dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-11-02 11:51:04--  https://data.smgov.net/api/views/tsas-mvez/rows.csv?accessType=DOWNLOAD\n",
      "Resolving data.smgov.net... 52.206.68.26\n",
      "Connecting to data.smgov.net|52.206.68.26|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: unspecified [text/csv]\n",
      "Saving to: ‘service.csv’\n",
      "\n",
      "service.csv             [         <=>        ]   5.71M  1.15MB/s    in 5.3s    \n",
      "\n",
      "Last-modified header invalid -- time-stamp ignored.\n",
      "2016-11-02 11:51:09 (1.07 MB/s) - ‘service.csv’ saved [5988947]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://data.smgov.net/api/views/tsas-mvez/rows.csv?accessType=DOWNLOAD -O service.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!mkdir prj2data\n",
    "!mv service.csv ./prj2data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1 - examine the data (20 points)\n",
    "\n",
    "Use `csvstat`, `pandas`, or other tools as you see fit and observe the contents of your dataset.  Review important points from any metadata provided.  Describe what you see:  What do the columns mean?  Are there null values?  Which columns interest you the most?  Which columns present some opportunities, challenges, or questions you would like to ask?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request ID,Topic,Assigned Department,Status,Request Date,Response Date,Days to Respond,Latitude,Longitude,Location\r\n",
      "1993315,Public Records Request,City Clerk,Closed,12/15/2014,12/17/2014,3,,,\r\n"
     ]
    }
   ],
   "source": [
    "!head -n2 ./prj2data/service.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import psycopg2 as pg\n",
    "%load_ext sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./prj2data/service.csv', parse_dates=['Request Date', 'Response Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58268, 10)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do the columns mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Request ID                      int64\n",
       "Topic                          object\n",
       "Assigned Department            object\n",
       "Status                         object\n",
       "Request Date           datetime64[ns]\n",
       "Response Date          datetime64[ns]\n",
       "Days to Respond               float64\n",
       "Latitude                      float64\n",
       "Longitude                     float64\n",
       "Location                       object\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names are self-explanatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Request ID                 0\n",
       "Topic                      0\n",
       "Assigned Department      696\n",
       "Status                     0\n",
       "Request Date               0\n",
       "Response Date              3\n",
       "Days to Respond            3\n",
       "Latitude               30723\n",
       "Longitude              30723\n",
       "Location               30723\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which columns interest you the most?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic, Assigned Department and Days to Respond are the most interesting columns to me. I would like to find out hwo effecient each department, in term of days, handle their service requests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Which columns present some opportunities, challenges, or questions you would like to ask?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latitude and Longitude have a lot of missing values. Maybe the location data are not relevant for many services. And It is hard to find free service to convert latitudes and longitudes to addresses to enrich the location dimension. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2 - define a database model, load the data, and explore (20 points)\n",
    "\n",
    "Based on what you found above, create and connect to a new database, define a database table in it, and load this dataset into it.  You may use either of the methods for this step you have seen in earlier class notebooks.  You may choose to eliminate variables/columns if they are not relevant or interesting to you - explain your reasoning if you do.  Either way, you should load a majority of the columns present in the source dataset as it is in its raw form into the database, and all of its rows.\n",
    "\n",
    "Once your data has loaded successfully, run a `COUNT(*)` query to verify that all the data has loaded correctly.\n",
    "\n",
    "Explore the data to zero in on a few themes you would like to further study analytically.  Discuss columns that present opportunities for extraction into dimensions, and identify the specific columns that contain facts you want to measure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!createdb prj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Connected: weizheng@prj2'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%sql postgresql://weizheng@localhost:5432/prj2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "Create Table service(\n",
    "    request_id integer,\n",
    "    topic varchar(50),\n",
    "    assigned_department varchar(50),\n",
    "    status varchar(50),\n",
    "    request_date date,\n",
    "    response_date date,\n",
    "    days_to_respond real,\n",
    "    latitude float8,\n",
    "    longitude float8,\n",
    "    primary key(request_id)\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#drop location because it can be obtained by combining latitude and longtitude\n",
    "df.drop('Location', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/weizheng/msba/data_management/istm-6212\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./prj2data/tosql.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58268 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "COPY service FROM '/Users/weizheng/msba/data_management/istm-6212/prj2data/tosql.csv'\n",
    "CSV\n",
    "HEADER\n",
    "DELIMITER ',';"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 rows affected.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <th>count</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td>58268</td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "[(58268,)]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sql\n",
    "Select Count(*)\n",
    "From service;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3 - define a star schema, and load your data into it (30 points)\n",
    "\n",
    "Define a star schema consisting of at least one fact table and at least two dimensions.  Transform your source data into the new fact table and dimensions.  Discuss your key modeling decisions.\n",
    "\n",
    "Document your star schema with a diagram.  There several tools available for this, such as Google Docs and Microsoft Visio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4 - explore your data in its new schema (30 points)\n",
    "\n",
    "Strictly using the dimensional model tables you defined and populated for Problem 3, explore your data in its new form.  Use any of the query strategies we've seen in class, including transformations, aggregates, subqueries, rollups, and, of course, joins.  Add plots to highlight particular themes that stand out.  Describe your thinking and observations along the way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus (10 points for one of A or B)\n",
    "\n",
    "\n",
    "### Option A - Automating ETL\n",
    "\n",
    "Consider the work you did to load your original raw dataset and then transform it into a dimensional model for analysis.  What would it take to automate this process?  How often would you need to update the data?  Could you easily automate any data cleaning steps?  What checks would you need to put in place to ensure quality?  Would humans need to be involved, or could you automate it all?\n",
    "\n",
    "Discuss.\n",
    "\n",
    "\n",
    "### Option B - Augmenting dimensions\n",
    "\n",
    "Considering your dimensional model, what external data could you find to augment it?  As in the case of bike trips, where weather might provide an interesting added dimension/context not present in the original data, you can probably find another source of data to complement your own model.  Identify one such source and add it to your model, demonstrating its value with a few new queries."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
